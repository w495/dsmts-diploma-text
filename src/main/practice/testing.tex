%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%%% EVAL
%%%

\subsection{Тестирование разработанной ССМП}

\subsubsection{Оценка качества перевода}

Для оценки качества перевода достаточно взглянуть на таблицу ниже.
В ней приведены переводы одного и того же фрагмента для различных 
статистических систем машинного перевода.

\begin{dtable}{Различные переводы исходного текста}
		\begin{tabular}{|r|p{10cm}|}
			\hline  Исходный текст 		& adopted at the 81st plenary meeting \\ 
			\hline  Проф. переводчик 	& принята на 81-м пленарном заседании \\ 
			\hline  Moses 				& приняли на 81 полный встречи \\ 
			\hline  Google-переводчик 	& принятой на 81-е пленарное заседание \\ 
			\hline  Данная система 		& принята без голосования на 81 пленарном заседании в Брюсселе \\ 
			\hline 
	\end{tabular} 
\end{dtable}

Все правильно, заседании было в Брюсселе, но в исходном тексте этого 
не~было. Однако не совсем корректно сравнивать системы, обученные на разных
корпусах текстов. Потому мы будем использовать систему машинного перевода
основанную на пакете Moses (декодировщик Moses и обучающая система GIZA++),
обученную на том же самом входном корпусе, что и наша система.

Для оценки качества перевода существует набор формальных метрик.
\begin{itemize}
	\item BLEU (Bilingual Evaluation Understudy);
	\item METEOR (Metric for Evaluation of Translation with Explicit ORdering);
	\item NIST (названа по имени университета National Institute of Standards and Technology);
	\item WER (Word error rate).
\end{itemize}

Чаще всего системы машинного перевода оценивают по метрике BLEU:
\begin{itemize}
	\item высокая корреляция с оценкой человека;
	\item для оценки требуется готовый перевод тестовых предложений, выполненный профессиональным переводчиком.
\end{itemize}

\[
	\mathrm{BLEU} = Bp \cdot e^{ \left(  \sum\limits_{n=1}^{N} W_n \log(p_n) \right) }
\]\[
	Bp = \left\lbrace 
		\begin{array}{cc}
			1, &  l_c > l_h;\\ 
			e^{\left(1 - \frac{l_h}{l_c}\right)}, &  l_c \le l_h.\\ 
		\end{array} 
	\right.
	\quad \text{ и }\quad
	p_n =
		\dfrac{\sum\limits_{C \in S_c}  \sum\limits_{\eta_c \in C} \text{\it число}_{\text{среза}}(\eta_c) }
			{\sum\limits_{C \in S_c}  \sum\limits_{\eta_c \in C} \text{\it число}(\eta_c) }
\]

\begin{itemize}
	\item $S_c$ --- множество кандидатов на перевод;
	\item $C$ --- кандидат на перевод; 
	\item $\eta_c$ --- $n$-грамма кандидата на перевод;
	\item $l_c$ --- длинна кандидата перевода;
	\item $l_h$ --- длинна экспертного перевода (выполненного человеком);
	\item $W_n = \dfrac{1}{N}$ --- вес, для метрик основанных на BLEU, но не эквивалентных ей, может отличаться для каждого $n$, например NIST назначает больший вес более редким $n$-граммам;
	\item $N = 4$, $n$-грамность оценки.
\end{itemize}

Таким образом, BLEU есть взвешенное среднее числа совпадений
$n$-грамм (слов) кандидата и $n$-грамм в переводе эксперта. 
Метрика является инвариантом порядка $n$-грамм, важно наличие совпадений \cite{Кан:2011}. 
Чем выше значение метрики BLEU, тем перевод хуже.

Метрики NIST и METEOR основаны на BLEU, WER --- на вычислении расстояния Левенштейна.

Для оценки перевода системы мы использовали метрику BLEU.
Для подсчета метрики было выделено 1024 предложений в качестве примеров экспертного перевода, 
и 1024 предложений было переведено рассматриваемой системой МП. 
Для этого набора предложений метрика BLEU = 0.243 для единственной итерации работы декодера,
и 0.209 для лучшего варианта из 100  полученного в результате нескольких итераций.
Система МП, построенная на основе пакета Moses,
 дает метрику BLEU = 0.209 для модели IBM 3 и BLEU = 0.173 для модели IBM 5.

\begin{dtable}{Оценки перевода текста}
	\begin{tabular}{|r|r|}
		\hline  \textbf{Система}	&	\textbf{BLEU}  \\ 
		\hline  Текущая (1)			&	0.243 \\ 
		\hline  Текущая (100)		&	0.209 \\ 
		\hline  Moses (IBM 3)		&	0.201 \\ 
		\hline  Moses (IBM 5) 		& 	0.173 \\ 
		\hline 
	\end{tabular} 
\end{dtable}

Высокие значения метрики BLEU показывают, что система проявляет 
себя не в лучшем свете на тестовом корпусе, однако сам корпус
весьма отличается отличается от тех для которых создана данная система.

Существует целый ряд неформальных способов оценки систем машинного перевода.
Чаще всего используют технику <<обратного перевода>>. 
\begin{itemize}
	\item исходный отрывок переводят системой c языка $L_1$ на язык $L_2$;
	\item полученный отрывок переводят в обраПритную сторону c языка $L_2$ на язык $L_1$.
	\item сравнивают два варианта отрывка на языке 	$L_1$ и по их близости судят о качестве перевода.
\end{itemize}
На наш взгляд такой метод оценки не является вполне корректным в случае статистических систем.
Так как согласно уравнению Байеса, в итоге мы оцениваем только модель языка используемой системы.
Более того при многократном применении <<обратного перевода>> к одному и тому же отрывку,
с некоторой итерации перевод на $L_1$ и на $L_2$ перестанут меняться.

\pagebreak

\subsubsection{Оценка скорости}

\paragraph{Оценка скорости обучения}

Ниже мы сравним скорости обучения и работы трех систем машинного перевода.
Для того чтобы показать разницу распределенных (многопоточных) и не распределенных систем
эксперименты проводились на разных машинах.
Для тестирования рассматриваемой системы было на каждой машине запускались 
соответствующие приложения (читатель и обработчик), пропорционально числу ядер процессоров.
Тестирование как и ранее проводилось на смешанном корпусе описанном выше.

Высокая скорость текущей (представленной в работе) системы 
во многом объясняется ее простотой. 
Однако, для более крупных корпусов текста, скорость обучения может оказаться критичной.

\textbf{Оценка скорости обучения на одном и том же корпусе.} \\

\begin{dtable}{Оценка скорости обучения, процессор: Intel Core2 Duo, 1 ядро 64 бит, ОП 4Гб, ФС:~ext4}
	\begin{tabular}{|r|r|}
		\hline  \textbf{Система}		& \textbf{Время, ч} \\  
		\hline  Текущая (1)				&	$ \approx $ 5   \\ 
		\hline  Moses (GIZA++)			&	$ \approx $ 25  \\ 
		\hline  Chaski (MGIZA++)		&	$ \approx $ 26  \\ 
		\hline 
	\end{tabular} 
\end{dtable}

\begin{dtable}{Оценка скорости обучения, процессор: Intel Xeon E5506, 8 ядер 64 бит, ОП 10Гб, ФС:xfs}
	\begin{tabular}{|r|r|}
		\hline  \textbf{Система}	& \textbf{Время, ч} \\ 
		\hline  Текущая (1)			&	$ \approx $ 1 	\\
		\hline  Moses (GIZA++)		&	$ \approx $ 22 	\\
		\hline  Chaski (MGIZA++)	&	$ \approx $ 3 	\\ 
		\hline 
	\end{tabular} 
\end{dtable}

\paragraph{Оценка скорости перевода}

Скорость перевода будем оценивать в микросекундах. 
Причем, скорость запуска декодера (Moses стартует в течение минуты на Intel Core2 Duo) 
и время на прочие накладные вычисления учитывать не будем. \\

\begin{dtable}{Оценка скорости перевода, процессор: Intel Core2 Duo, 1 ядро 64 бит, ОП 4Гб, ФС: ext4}
	\begin{tabular}{|r|r|}
		\hline  \textbf{Система}	&	Время, $\mu$с \\ 
		\hline  Текущая (1)			&	1132 \\ 
		\hline  Текущая (100)		&	7108124  \\ 
		\hline  Moses (IBM 3)		&	$\approx$ 10000000\\ 
		\hline  Moses (IBM 5) 		& 	$\approx$ 30000000 \\ 
		\hline 
	\end{tabular} 
\end{dtable}

\begin{dtable}{Оценка скорости перевода, процессор: Intel Xeon E5506, 8 ядер 64 бит, ОП 10Гб, ФС: xfs}
	\begin{tabular}{|r|r|}
		\hline  \textbf{Система}	&	Время, $\mu$с \\ 
		\hline  Текущая (1)			&	1012 \\ 
		\hline  Текущая (100)		&	1119024  \\ 
		\hline  Moses (IBM 3)		&	$\approx$ 5000000\\ 
		\hline  Moses (IBM 5) 		& 	$\approx$ 6000000 \\ 
		\hline 
	\end{tabular} 
\end{dtable}

Как можно заключить из приведенных тестов использование параллельных вычислений позволяет 
достичь значительного прироста производительности на этапе обучения системы машинного перевода.

На этапе декодирования особенный рост наблюдать трудно. 
Это будет весьма заметно на большом количестве одновременных запросов.
 
\pagebreak

