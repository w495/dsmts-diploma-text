\subsubsection{Обучение ССМП}

\paragraph{Вычисление языковой модели}

В качестве модели языка в системах статистического перевода используются преимущественно различные модификации n-граммной модели, утверждающей, что <<грамматичность>> выбора очередного слова при формировании текста определяется только тем, какие $(n – 1)$ слов идут перед ним. Вероятность каждого n-грамма определяется по его встречаемости в тренировочном корпусе \cite{Рахимбердиев:2003}.

\[
	P( \WA_1 \ldots \WA_{\LA} ) = \prod\limits_{i=0}^{i=\LA+n-1} P'(\WA_i|\WA_{i-1} \ldots \WA_{i-n+1})
\]
$n$ --- $n$-граммность модели.
\pagebreak
\[
	P'(\WA_m|\WA_1 \ldots \WA_{m-1}) = K_n \cdot P(\WA_m|\WA_1 \ldots \WA_{m-1}) +
\]\[
	\qquad +  K_{m-1} \cdot P(\WA_{m-1}|\WA_1 \ldots \WA_{m-2}) + 
\]\[
	\qquad + K_2 \cdot P(\WA_2|\WA_1) + K_1 \cdot P(\WA_1) + K_0;
\]
\[
	P(\WA_1) = \dfrac{\text{\it частота } (\WA_1)}{|\TA|};
\]\[
	P(\WA_m|\WA_1 \ldots \WA_{m-1}) = \dfrac{\text{\it частота } (\WA_1 \ldots \WA_{m-1}\WA_m)}
		{\text{\it частота } (\WA_1 \ldots \WA_{m-1})};
\]

$K_i$ --- коэффициенты сглаживания.
Они могут быть выбраны различными способами. 
Чаще всего используется линейная интерполяция.
\[
	K_i > K_{i + 1};
\]\[
	\sum\limits_{i= 0}^{i = n} K_i = 1.0;
\]
В этом случае придется подбирать и экспериментально, 
например для трехграммной модели
$K_3 = 0.8, K_2 = 0.15, K_1 = 0.049, K_0 = 0.001$ \cite{Кан:2011}

$P'$ можно вычислить иначе, используя адаптивный метод сглаживания
\[
	P'(\WA_m|\WA_1 \ldots \WA_{m-1}) = \dfrac{\delta + \text{\it частота } (\WA_1 \ldots \WA_m) }
		{\sum\limits_{i}\left( \delta  + \text{\it частота } (\WA_{1_j} \ldots \WA_{m_j}) \right) }
\]\[
		= \dfrac{\delta + \text{\it частота } (\WA_1 \ldots \WA_m) }
				{ \delta \cdot V + \sum\limits_{i}\left(\text{\it частота } (\WA_{1_j} \ldots \WA_{m_j}) \right) }
\]

$V$ – количество всех $n$-грамм в используемом корпусе.
Наиболее простым случаем аддитивного сглаживания является метод, 
когда $\delta=1$ – метод сглаживания Лапласа \cite{Романов:2008:1}.

Существуют и другие техники сглаживания вероятностей 
(Гуда-Тьюринга, Катца, Кнезера-Нейя \cite{Chen:1999}),

\pagebreak
\paragraph{Вычисление модели перевода}

Обозначим:
\begin{itemize}
	\item $\TE$ --- <<английский>> текст (множество предложений);
	\item $\TR$ --- <<русский>> текст;
	\item $\SE$ --- <<английское>> предложение (последовательность слов);
	\item $\SR$ --- <<русское>> предложение;
	\item $\WE$ --- <<английское>> слово;
	\item $\WR$ --- <<русское>> слово;
	\item $\LE 	\gets |\SE|	$; 
	\item $\LR 	\gets |\SR|	$;
	\item $\POSR 	\gets \text{ позиция } \WR \text{ в } \SR$; 
	\item $\POSE 	\gets \text{ позиция } \WE \text{ в } \SE$.
\end{itemize}
Пусть $P(\SE|\SR)$ --- вероятность некоторой строки (предложения) из $e$, 
при~гипотезе перевода из $r$.
По аналогии c моделью языка можно предположить, что
\[
	P(\SE|\SR) = \dfrac{\text{\it частота } (\SE,\SR)}{\text{\it частота } (\SR)};
\]
Однако это не верно.
Для вычисления модели перевода нужно:
\begin{itemize}
	\item разделить предложение на меньшие части;
	\item{ ввести новую переменную $\A$, представляющую 
		выравнивания между отдельными словами в паре предложений.
		\[
			P(\SE|\SR) = \sum\limits_{\A} P(\SE, \A|\SR);
		\]
	}
\end{itemize}
Вероятность перевода:
\[
	P(\SE, \A| \SR)	= \dfrac{\varepsilon}{(\LR + 1)^{\LE}} \prod\limits_{j = 1}^{\LE} t(\WE_{j} | \WR_{\A(j)}) 
\]
$t$ – это вероятность слова оригинала в позиции $j$ при соответствующем 
ему слове перевода $\WR_{\A(j)}$, определенном выравниванием $\A$. 
$\varepsilon$ --- нормализующая <<константа>>.
В этой работе $\varepsilon$ выбирается равным $1$, но если рассуждать более строго, 
$\varepsilon$ --- распределение вероятностей длин предложений каждого из~языков
\[
	\varepsilon = \varepsilon(\LE|\LR)
\]

Для приведения $P(\SE, \A| \SR)$ к $P(\A | \SE, \SR)$, 
т.е. к вероятности данного выравнивания при данной паре предложений, 
каждая вероятность $P(\SE, \A| \SR)$ нормализуется по сумме вероятностей 
всех выравниваний данной пары предложений:
\[
	P(\A | \SE, \SR) = \dfrac{P(\SE, \A| \SR)}
		{\sum\limits_{\A} P(\SE, \A| \SR)}
\]

Имея набор выравниваний с определенными вероятностями, 
можно подсчитать частоты каждой пары слов, 
взвешенные по вероятности выравниваний, в~которых они встречаются. 
Например, если какая-то пара слов встречается в двух выравниваниях, 
имеющих вероятности $0.5$ и 1, то взвешенная частота ($counts$) 
такой пары равна $1.5$ \cite{Рахимбердиев:2003}.
\[
	t(\WE|\WR) 
		=  \dfrac{counts(\WE|\WR)}{\sum\limits_{\WE}counts(\WE|\WR)} 
		= \dfrac{counts(\WE|\WR)}{total(\WR)};
\]

Требуется оценить вероятности лексического перевода $t(\WE|\WR)$ 
из параллельного корпуса $(\TE, \TR)$.
Но чтобы сделать это нужно вычислить $\A$, которой у нас нет.
Возникает так называемая <<проблема курицы и яйца>>.
Для оценки параметров модели нужно знать выравнивания.
Для оценки выравнивания нужно знать параметры модели.

\pagebreak
При решении этой проблемы используют EM-алгоритм (Витерби), 
который более детально рассмотрен в приложении. EM-алгоритм:
\begin{enumerate}
	\item Инициализируем параметры модели (одинаковыми значениями, на первой итерации);
	\item Оценим вероятности отсутствующей информации;
	\item Оценим параметры модели на основании новой информации;
	\item Перейдем к следующей итерации.
\end{enumerate}

% Более детально алгоритм рассмотрен в приложении 1.

